
### Software Stack 

Our entire software stack will influence our performance metrics. 

!!! todo 
    Diagram 

!!! note
    Let us look at some common errors as examples.

### Unwanted Overhead
    
Its tempting to write or compile a program and benchmark it directly from your shell:

```bash
time python -c "import time; time.sleep(.1)"
```

!!! note
    We can naively measure a program directly from our shell.

Or look at your "wallclock".

```output
real    0m0,163s
user    0m0,042s
sys     0m0,014s
```

We can see that we are measuring some overhead when we do this. 

!!! note
    As you can see from the output of that command: this is not ideal.

    If you were measuring a simple C program the measurement would (probably) include the time it takes for the operating system to start the process and some overhead by the c binary itself like linking libraries.
    Because we are measuring a python program, this will include all sorts of whacky things we dont need to worry about.

    _Loading and starting the interpreter, loading program files from disk, parsing them, writing byte code to disk for caching ..._

    In this case... the `import time` statement.

::: center
| Name | Definition |
|------|------------|
| real | Elapsed real (wall clock) time used by the process. |
| user | Total number of CPU-seconds that the process used directly (in user mode), in seconds. |
| sys  | Total number of CPU-seconds used by the system on behalf of the process (in kernel mode), in seconds. |    
:::

!!! note
    real vs. user vs. sys

    Something very useful however, is to see how much time the program has spent in the kernel ("system").
    All the time used by sys-calls is listed on its own which might indicate performance problems connected to writing files, listening on the network and more.

    Notice that our "user time" is very low. This is because `time.sleep` doesn't really use the CPU, we are not "CPU-bound".


Find out more with: `man time`

!!! note
    Find out more on the man page.

::: center
| Name | Definition |
|------|------------|
| real | If you were very good at timing with a stopwatch, this is what you would get. |
| user | Amount of time *actually* spent in your code. This does **not include** time spent *blocked* or in other processes. |
| sys  | Time spent when doing things only **the kernel** can do. Allocating *memory*, using the *file system* and so on ... |
:::

!!! note
    Translation into common english. 

```bash
time python -c "for i in range(10_000): i**i"
```

```output
real    0m3,163s
user    0m3,151s
sys     0m0,004s
```

!!! note
    If we actually start using our CPU, our user time will match our real time.

### Hard-To-Predict Noise

Your computer is constantly occupied with ubiquitous and indispensible tasks.

!!! note
    Measuring programs that are "too fast".

    When measuring performance of a program that is "too fast", you might end up measuring a siginifcant error introduced by (for example) process scheduling in the operating system. Other overhead like GCs, file-systems, etc. might also play into this. As a rule of thumb: the more high-level your environment the more error will be introduced by these factors.

Benchmarking tests *must* be slow since you *must* do statistics on multiple results.

!!! note 
    This means benchmarking tests are pretty much /always/ slow unless you are using a real-time operating system.
    Real-time scheduling features now exist in the vanilla Linux.

    **Back in my day** _(a year ago) we had to install a custom version of the linux kernel to enjoy real time scheduling features. For example for inter-programm pro audio communication._


```bash
for N in $(seq -s" " 10);
do
   pytest -k test_python_too_fast -q
done
```

!!! note
    Lets take a look...

Results measured beforehand on `python 3.10`. These are individual test runs concatenated into one table.

::: center

| Name  (time in us)       | Mean      | OPS (Kops/s)  | Rounds | Iterations |
|--------------------------|-----------|---------------|--------|------------|
| test_python_too_fast     | 202.7160  | 4.9330        | 1      | 1          |
| test_python_too_fast     | 199.1040  | 5.0225        | 1      | 1          |
| test_python_too_fast     | 254.6150  | 3.9275        | 1      | 1          |
| test_python_too_fast     | 246.1540  | 4.0625        | 1      | 1          |
| test_python_too_fast     | 221.9280  | 4.5060        | 1      | 1          |
| test_python_too_fast     | 425.5660  | 2.3498        | 1      | 1          |
| test_python_too_fast     | 212.9930  | 4.6950        | 1      | 1          |
| test_python_too_fast     | 219.2340  | 4.5613        | 1      | 1          |
| test_python_too_fast     | 250.9560  | 3.9848        | 1      | 1          |
| test_python_too_fast     | 216.3560  | 4.6220        | 1      | 1          |

:::

!!! note
    There is an outlier.
        
    As you can see there is an outlier that is close to 100% slower than the average runtime otherwise.
    Even worse: if you are not an expert on the scheduling algorithm of your operating system it is basically impossible to know or measure how big or how frequent the error will be.


```bash
pytest -q -k test_python_mm
```

!!! note
    Statistics can fix this. 

    To fix this, most benchmarking libraries will automatically run benchmarks multiple times and present statistics of the result.

```output
...                                                                                                                                                                            [100%]

-------------------------------------------------------------------------------------- benchmark: 3 tests --------------------------------------------------------------------------------------
Name (time in ms)                 Min                 Max                Mean            StdDev              Median               IQR            Outliers      OPS            Rounds  Iterations
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_python_mm[A0-B0-R0]      10.9680 (1.0)       12.9500 (1.0)       11.2050 (1.0)      0.2154 (1.0)       11.1811 (1.0)      0.1286 (1.0)           7;4  89.2457 (1.0)          88           1
test_python_mm[A1-B1-R1]      86.4523 (7.88)      89.5111 (6.91)      88.1906 (7.87)     0.8719 (4.05)      88.2343 (7.89)     1.0413 (8.10)          3;0  11.3391 (0.13)         10           1
test_python_mm[A2-B2-R2]     297.3174 (27.11)    299.1777 (23.10)    297.9407 (26.59)    0.7296 (3.39)     297.7366 (26.63)    0.7750 (6.03)          1;0   3.3564 (0.04)          5           1
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
3 passed, 76 deselected in 5.20s
```

!!! todo
    ... maybe make it more relateable by using the same benchmarking function? 

If we were using `node`, we would also have to account for the JIT.

!!! note 
    JIT compilers etc.


```bash
pytest -q -k test_python_mm --benchmark-histogram --benchmark-autosave
```

---

- MIT 6.172 Performance Engineering of Software Systems: [10. Measurement and Timing](https://www.youtube.com/watch?v=LvX3g45ynu8)
- Stackoverflow: [What do 'real', 'user' and 'sys' mean in the output of time(1)?](https://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1/556411#556411)
- lurklurk.org: [Linkers](https://www.lurklurk.org/linkers/linkers.html)
- Wikipedia: [Real-time operating system](https://en.wikipedia.org/wiki/Real-time_operating_system)
- ZDNet: [20 years later, real-time Linux makes it to the kernel - really](https://www.zdnet.com/article/20-years-later-real-time-linux-makes-it-to-the-kernel-really/)
- pypi.org: [pytest-benchmark](https://pypi.org/project/pytest-benchmark/)
